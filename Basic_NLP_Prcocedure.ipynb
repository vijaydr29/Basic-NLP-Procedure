{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vijaydr29/Basic-NLP-Procedure/blob/main/Basic_NLP_Prcocedure.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Agenda**"
      ],
      "metadata": {
        "id": "R0i8JQEO4kid"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "#### 1. **NLP Overview:**\n",
        "   - Definition and Importance\n",
        "   - Key Challenges\n",
        "\n",
        "#### 2. **How to Process Data:**\n",
        "   - Text Cleaning\n",
        "   - Tokenization\n",
        "   - Stopword Removal\n",
        "   - Lemmatization\n",
        "   \n",
        "\n",
        "#### 3. **How ML Models Work:**\n",
        "   - Introduction to Machine Learning\n",
        "   - Text Representation\n",
        "      - Bag of Words\n",
        "      - TF-IDF\n",
        "      - Word Embeddings (Word2Vec)\n",
        "\n",
        "\n",
        "#### 4. **Word Embeddings (Word2Vec):**\n",
        "   - Word2Vec Basics\n",
        "   - Gensim Library for Word2Vec\n",
        "\n",
        "\n",
        "#### 5. **Seq2Seq Models:**\n",
        "   - Introduction to Seq2Seq\n",
        "   - Encoder-Decoder Architecture\n",
        "\n",
        "\n",
        "#### 6. **Transformer Models with Attention:**\n",
        "   - Introduction to Transformers\n",
        "   - Self-Attention Mechanism\n",
        "   - BERT (Bidirectional Encoder Representations from Transformers)\n"
      ],
      "metadata": {
        "id": "dmbT48IUL2tg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## **1. NLP Overview: Definition and Importance**"
      ],
      "metadata": {
        "id": "cK1KubiD4S6c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "#### **1.1 Definition:**\n",
        "\n",
        "NLP, or Natural Language Processing, is a subfield of artificial intelligence (AI) that focuses on the interaction between computers and human language. The goal is to enable machines to understand, interpret, and generate human-like text.\n",
        "\n",
        "In practical terms, NLP involves the development of algorithms and models that can:\n",
        "\n",
        "- Understand the meaning behind a piece of text.\n",
        "- Extract relevant information from unstructured data.\n",
        "- Generate coherent and contextually relevant text.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "RwVL8kYhNCCo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **1.2 Importance of NLP:**\n",
        "\n",
        "NLP plays a crucial role in various applications, contributing to advancements in technology and improving user experiences. Some key areas where NLP is essential include:\n",
        "\n",
        "- **Chatbots and Virtual Assistants:** NLP powers conversational agents, allowing users to interact with machines in a natural language.\n",
        "\n",
        "- **Information Extraction:** NLP helps extract structured information from unstructured text, such as named entities, relationships, and events.\n",
        "\n",
        "- **Sentiment Analysis:** Businesses use NLP to analyze customer sentiments expressed in reviews, social media, or surveys.\n",
        "\n",
        "- **Language Translation:** NLP is behind machine translation systems that enable the translation of text from one language to another.\n",
        "\n",
        "- **Speech Recognition:** NLP algorithms are used to convert spoken language into written text, facilitating voice-activated systems.\n",
        "\n"
      ],
      "metadata": {
        "id": "jZKGReRnKyVC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **1.3 Key Challenges in NLP:**\n",
        "\n",
        "Despite the advancements, NLP faces several challenges due to the complexity and nuances of human language. Some key challenges include:\n",
        "\n",
        "- **Ambiguity:** Words and phrases often have multiple meanings based on context, making it challenging for machines to accurately interpret.\n",
        "\n",
        "- **Sarcasm and Irony:** Detecting sarcasm, irony, and other forms of figurative language is difficult for NLP systems, as it requires understanding contextual cues.\n",
        "\n",
        "- **Lack of Context:** Understanding context is essential for accurate language comprehension. NLP models may struggle when faced with ambiguous or incomplete information.\n",
        "\n",
        "- **Data Limitations:** NLP models heavily rely on large amounts of labeled data for training. Limited or biased datasets can lead to biased and less accurate models.\n",
        "\n"
      ],
      "metadata": {
        "id": "rdKF37RuK2n3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Real-life Example:**\n",
        "\n",
        "To illustrate the importance of NLP, consider a social media sentiment analysis application. The goal is to determine the sentiment (positive, negative, or neutral) of user comments about a product. NLP techniques, such as tokenization and sentiment analysis models, can be employed to automatically analyze and categorize user sentiments.\n"
      ],
      "metadata": {
        "id": "fiu1Z5vCK5so"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  importing the \"drive\" module from the \"google.colab\" library, facilitating access to Google Drive within a Colab notebook.\n",
        "from google.colab import drive"
      ],
      "metadata": {
        "id": "l6AzVb0FDC-2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# mounting the user's Google Drive to the \"/content/drive\" directory in a Google Colab notebook, enabling access to files and data stored on Google Drive within the Colab environment.\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BwqE1tW_DIbf",
        "outputId": "3c0c95f2-8049-4e78-c5a0-f89ba8b1ee43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# reading a CSV file named \"data.csv\" located in the \"Data\" folder on the your Google Drive and stores it as a DataFrame df\n",
        "# write your data file path\n",
        "import pandas as pd\n",
        "df = pd.read_csv('/content/drive/MyDrive/Master Class/IMDB Dataset.csv')"
      ],
      "metadata": {
        "id": "5sdfuoB7DRFj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "DK6sWlU1DkJi",
        "outputId": "0caaf644-7519-4e24-b9cb-feaf39ea4745"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                              review sentiment\n",
              "0  One of the other reviewers has mentioned that ...  positive\n",
              "1  A wonderful little production. <br /><br />The...  positive\n",
              "2  I thought this was a wonderful way to spend ti...  positive\n",
              "3  Basically there's a family where a little boy ...  negative\n",
              "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0ad74868-9f6a-4093-aee9-6b97c606cf84\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>One of the other reviewers has mentioned that ...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I thought this was a wonderful way to spend ti...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Basically there's a family where a little boy ...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0ad74868-9f6a-4093-aee9-6b97c606cf84')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-0ad74868-9f6a-4093-aee9-6b97c606cf84 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-0ad74868-9f6a-4093-aee9-6b97c606cf84');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-2a2b53a3-e54a-4101-9012-0bf37b105843\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-2a2b53a3-e54a-4101-9012-0bf37b105843')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-2a2b53a3-e54a-4101-9012-0bf37b105843 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the TextBlob class from the textblob library\n",
        "from textblob import TextBlob\n",
        "\n",
        "# Assuming you have already mounted Google Drive and loaded the IMDB dataset into a DataFrame df\n",
        "\n",
        "# Limit the analysis to the first 100 reviews\n",
        "num_reviews_to_analyze = 100\n",
        "\n",
        "# Initialize variables for accuracy calculation\n",
        "correct_predictions = 0\n",
        "\n",
        "# Iterate through the first 100 reviews in the dataset\n",
        "for index, row in df.head(num_reviews_to_analyze).iterrows():\n",
        "    # Extract the review text from the 'review' column\n",
        "    review_text = row['review']\n",
        "\n",
        "    # Create a TextBlob object for the current review\n",
        "    analysis = TextBlob(review_text)\n",
        "\n",
        "    # Calculate the sentiment polarity of the review\n",
        "    sentiment_polarity = analysis.sentiment.polarity\n",
        "\n",
        "    # Convert polarity to predicted sentiment label\n",
        "    predicted_sentiment = 'positive' if sentiment_polarity > 0 else 'negative' if sentiment_polarity < 0 else 'neutral'\n",
        "\n",
        "    # Compare with the true sentiment label\n",
        "    true_sentiment = row['sentiment']\n",
        "\n",
        "    # Check if prediction is correct\n",
        "    if predicted_sentiment == true_sentiment:\n",
        "        correct_predictions += 1\n",
        "\n",
        "    # Print the sentiment polarity and predicted sentiment of the review\n",
        "    print(f\"Review {index + 1}: Polarity={sentiment_polarity}, Predicted Sentiment={predicted_sentiment}, True Sentiment={true_sentiment}\")\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = correct_predictions / num_reviews_to_analyze\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n"
      ],
      "metadata": {
        "id": "BOTcPMIosKIP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0177e121-3bc4-4ec8-a3ee-c489b0290587"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Review 1: Polarity=0.023433179723502305, Predicted Sentiment=positive, True Sentiment=positive\n",
            "Review 2: Polarity=0.1097222222222222, Predicted Sentiment=positive, True Sentiment=positive\n",
            "Review 3: Polarity=0.35400793650793644, Predicted Sentiment=positive, True Sentiment=positive\n",
            "Review 4: Polarity=-0.0578125, Predicted Sentiment=negative, True Sentiment=negative\n",
            "Review 5: Polarity=0.2179522497704316, Predicted Sentiment=positive, True Sentiment=positive\n",
            "Review 6: Polarity=0.15529411764705883, Predicted Sentiment=positive, True Sentiment=positive\n",
            "Review 7: Polarity=0.2855218855218855, Predicted Sentiment=positive, True Sentiment=positive\n",
            "Review 8: Polarity=0.08271604938271605, Predicted Sentiment=positive, True Sentiment=negative\n",
            "Review 9: Polarity=-0.1428628389154705, Predicted Sentiment=negative, True Sentiment=negative\n",
            "Review 10: Polarity=0.41500000000000004, Predicted Sentiment=positive, True Sentiment=positive\n",
            "Review 11: Polarity=0.12738095238095237, Predicted Sentiment=positive, True Sentiment=negative\n",
            "Review 12: Polarity=0.09920634920634923, Predicted Sentiment=positive, True Sentiment=negative\n",
            "Review 13: Polarity=0.07353896103896104, Predicted Sentiment=positive, True Sentiment=negative\n",
            "Review 14: Polarity=0.004910714285714289, Predicted Sentiment=positive, True Sentiment=negative\n",
            "Review 15: Polarity=0.39, Predicted Sentiment=positive, True Sentiment=positive\n",
            "Review 16: Polarity=-0.046666666666666655, Predicted Sentiment=negative, True Sentiment=negative\n",
            "Review 17: Polarity=0.23813492063492064, Predicted Sentiment=positive, True Sentiment=positive\n",
            "Review 18: Polarity=-0.16298076923076923, Predicted Sentiment=negative, True Sentiment=negative\n",
            "Review 19: Polarity=0.09642857142857143, Predicted Sentiment=positive, True Sentiment=positive\n",
            "Review 20: Polarity=0.1892156862745098, Predicted Sentiment=positive, True Sentiment=negative\n",
            "Review 21: Polarity=0.05215517241379311, Predicted Sentiment=positive, True Sentiment=positive\n",
            "Review 22: Polarity=-0.1618333333333333, Predicted Sentiment=negative, True Sentiment=negative\n",
            "Review 23: Polarity=0.2642857142857143, Predicted Sentiment=positive, True Sentiment=positive\n",
            "Review 24: Polarity=-0.02510748510748509, Predicted Sentiment=negative, True Sentiment=negative\n",
            "Review 25: Polarity=0.039692982456140356, Predicted Sentiment=positive, True Sentiment=negative\n",
            "Review 26: Polarity=0.06963541666666667, Predicted Sentiment=positive, True Sentiment=positive\n",
            "Review 27: Polarity=0.18794263496644448, Predicted Sentiment=positive, True Sentiment=positive\n",
            "Review 28: Polarity=-0.06759259259259259, Predicted Sentiment=negative, True Sentiment=negative\n",
            "Review 29: Polarity=-0.0732142857142857, Predicted Sentiment=negative, True Sentiment=negative\n",
            "Review 30: Polarity=-0.021323529411764696, Predicted Sentiment=negative, True Sentiment=positive\n",
            "Review 31: Polarity=0.014071969696969696, Predicted Sentiment=positive, True Sentiment=positive\n",
            "Review 32: Polarity=0.11227477477477481, Predicted Sentiment=positive, True Sentiment=positive\n",
            "Review 33: Polarity=0.019871794871794853, Predicted Sentiment=positive, True Sentiment=negative\n",
            "Review 34: Polarity=0.14265057621222005, Predicted Sentiment=positive, True Sentiment=positive\n",
            "Review 35: Polarity=0.02719298245614035, Predicted Sentiment=positive, True Sentiment=negative\n",
            "Review 36: Polarity=0.0243431855500821, Predicted Sentiment=positive, True Sentiment=negative\n",
            "Review 37: Polarity=-0.38958333333333334, Predicted Sentiment=negative, True Sentiment=negative\n",
            "Review 38: Polarity=0.0466765873015873, Predicted Sentiment=positive, True Sentiment=negative\n",
            "Review 39: Polarity=0.4161424512987013, Predicted Sentiment=positive, True Sentiment=positive\n",
            "Review 40: Polarity=-0.1720627325890484, Predicted Sentiment=negative, True Sentiment=negative\n",
            "Review 41: Polarity=0.17916666666666664, Predicted Sentiment=positive, True Sentiment=negative\n",
            "Review 42: Polarity=0.32275641025641033, Predicted Sentiment=positive, True Sentiment=positive\n",
            "Review 43: Polarity=-0.10548340548340547, Predicted Sentiment=negative, True Sentiment=negative\n",
            "Review 44: Polarity=0.12791666666666668, Predicted Sentiment=positive, True Sentiment=negative\n",
            "Review 45: Polarity=0.13842592592592592, Predicted Sentiment=positive, True Sentiment=positive\n",
            "Review 46: Polarity=0.16928374655647388, Predicted Sentiment=positive, True Sentiment=positive\n",
            "Review 47: Polarity=0.3625, Predicted Sentiment=positive, True Sentiment=negative\n",
            "Review 48: Polarity=0.20000000000000004, Predicted Sentiment=positive, True Sentiment=negative\n",
            "Review 49: Polarity=0.14517077664399092, Predicted Sentiment=positive, True Sentiment=positive\n",
            "Review 50: Polarity=0.0149122807017544, Predicted Sentiment=positive, True Sentiment=negative\n",
            "Review 51: Polarity=0.07386937557392104, Predicted Sentiment=positive, True Sentiment=positive\n",
            "Review 52: Polarity=0.20056128512010857, Predicted Sentiment=positive, True Sentiment=positive\n",
            "Review 53: Polarity=0.09166666666666666, Predicted Sentiment=positive, True Sentiment=positive\n",
            "Review 54: Polarity=0.3812152133580705, Predicted Sentiment=positive, True Sentiment=positive\n",
            "Review 55: Polarity=0.010439560439560446, Predicted Sentiment=positive, True Sentiment=negative\n",
            "Review 56: Polarity=0.06311111111111116, Predicted Sentiment=positive, True Sentiment=negative\n",
            "Review 57: Polarity=0.10572562358276642, Predicted Sentiment=positive, True Sentiment=negative\n",
            "Review 58: Polarity=0.1886671401515152, Predicted Sentiment=positive, True Sentiment=negative\n",
            "Review 59: Polarity=0.1639059054153394, Predicted Sentiment=positive, True Sentiment=positive\n",
            "Review 60: Polarity=0.290343300110742, Predicted Sentiment=positive, True Sentiment=positive\n",
            "Review 61: Polarity=0.12097718253968255, Predicted Sentiment=positive, True Sentiment=negative\n",
            "Review 62: Polarity=-0.12760416666666669, Predicted Sentiment=negative, True Sentiment=negative\n",
            "Review 63: Polarity=0.20017857142857146, Predicted Sentiment=positive, True Sentiment=positive\n",
            "Review 64: Polarity=0.115625, Predicted Sentiment=positive, True Sentiment=negative\n",
            "Review 65: Polarity=0.0851851851851852, Predicted Sentiment=positive, True Sentiment=negative\n",
            "Review 66: Polarity=0.15848039215686271, Predicted Sentiment=positive, True Sentiment=positive\n",
            "Review 67: Polarity=-0.05669973544973544, Predicted Sentiment=negative, True Sentiment=negative\n",
            "Review 68: Polarity=0.15555555555555556, Predicted Sentiment=positive, True Sentiment=negative\n",
            "Review 69: Polarity=0.16218434343434346, Predicted Sentiment=positive, True Sentiment=negative\n",
            "Review 70: Polarity=0.20717687074829932, Predicted Sentiment=positive, True Sentiment=negative\n",
            "Review 71: Polarity=0.11611374736374737, Predicted Sentiment=positive, True Sentiment=negative\n",
            "Review 72: Polarity=0.03606532356532357, Predicted Sentiment=positive, True Sentiment=negative\n",
            "Review 73: Polarity=-0.05405844155844155, Predicted Sentiment=negative, True Sentiment=positive\n",
            "Review 74: Polarity=0.14739583333333336, Predicted Sentiment=positive, True Sentiment=positive\n",
            "Review 75: Polarity=0.1266628873771731, Predicted Sentiment=positive, True Sentiment=negative\n",
            "Review 76: Polarity=0.11625, Predicted Sentiment=positive, True Sentiment=positive\n",
            "Review 77: Polarity=-0.07110215053763441, Predicted Sentiment=negative, True Sentiment=positive\n",
            "Review 78: Polarity=0.0037337662337662367, Predicted Sentiment=positive, True Sentiment=negative\n",
            "Review 79: Polarity=0.03108766233766233, Predicted Sentiment=positive, True Sentiment=negative\n",
            "Review 80: Polarity=0.22443064182194622, Predicted Sentiment=positive, True Sentiment=positive\n",
            "Review 81: Polarity=0.18038194444444444, Predicted Sentiment=positive, True Sentiment=positive\n",
            "Review 82: Polarity=-0.11022727272727272, Predicted Sentiment=negative, True Sentiment=negative\n",
            "Review 83: Polarity=0.04107289107289107, Predicted Sentiment=positive, True Sentiment=negative\n",
            "Review 84: Polarity=-0.07848214285714289, Predicted Sentiment=negative, True Sentiment=negative\n",
            "Review 85: Polarity=-0.5687500000000001, Predicted Sentiment=negative, True Sentiment=negative\n",
            "Review 86: Polarity=-0.11963577097505668, Predicted Sentiment=negative, True Sentiment=negative\n",
            "Review 87: Polarity=0.1, Predicted Sentiment=positive, True Sentiment=negative\n",
            "Review 88: Polarity=0.19055397727272727, Predicted Sentiment=positive, True Sentiment=negative\n",
            "Review 89: Polarity=-0.09444444444444444, Predicted Sentiment=negative, True Sentiment=negative\n",
            "Review 90: Polarity=-0.020014880952380944, Predicted Sentiment=negative, True Sentiment=negative\n",
            "Review 91: Polarity=0.19631944444444444, Predicted Sentiment=positive, True Sentiment=positive\n",
            "Review 92: Polarity=0.0019480519480519526, Predicted Sentiment=positive, True Sentiment=negative\n",
            "Review 93: Polarity=0.2460449735449735, Predicted Sentiment=positive, True Sentiment=positive\n",
            "Review 94: Polarity=0.10744318181818183, Predicted Sentiment=positive, True Sentiment=positive\n",
            "Review 95: Polarity=0.08293650793650793, Predicted Sentiment=positive, True Sentiment=negative\n",
            "Review 96: Polarity=-0.08255834204110067, Predicted Sentiment=negative, True Sentiment=positive\n",
            "Review 97: Polarity=0.0773809523809524, Predicted Sentiment=positive, True Sentiment=negative\n",
            "Review 98: Polarity=-0.20089613970588233, Predicted Sentiment=negative, True Sentiment=negative\n",
            "Review 99: Polarity=-0.3800595238095238, Predicted Sentiment=negative, True Sentiment=negative\n",
            "Review 100: Polarity=0.11050741792929292, Predicted Sentiment=positive, True Sentiment=positive\n",
            "Accuracy: 59.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## **2. How to Process Data:**"
      ],
      "metadata": {
        "id": "fUwgm7X039Uj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "#### **2.1 Text Cleaning:**\n",
        "\n",
        "Text cleaning involves removing unnecessary elements from the text, such as special characters or unwanted symbols."
      ],
      "metadata": {
        "id": "eftHDTn-uN67"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **2.2 Tokenization:**\n",
        "\n",
        "Tokenization is the process of breaking text into individual words or tokens.\n"
      ],
      "metadata": {
        "id": "7ycz9jlqKlwo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the Natural Language Toolkit (nltk) library\n",
        "import nltk\n",
        "\n",
        "# Download the Punkt tokenizer model (if not already downloaded)\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Import the word_tokenize function from the nltk.tokenize module\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Sample text\n",
        "sample_text = \"This is random text.\"\n",
        "\n",
        "# Tokenize the text using word_tokenize\n",
        "tokens = word_tokenize(sample_text)\n",
        "\n",
        "# Print the list of tokens\n",
        "print(tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p-AVUBXPwRJ1",
        "outputId": "8dcb0005-4936-462e-b6bd-f83bedda7572"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['This', 'is', 'random', 'text', '.']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation:**\n",
        "- `word_tokenize(text)`: Using the NLTK library, this function tokenizes the input text into a list of words.\n",
        "\n",
        "**Real-life Example:**  \n",
        "Consider a scenario where you want to analyze the frequency of words in a text document. Tokenization helps in breaking down the text into individual words for further analysis.\n",
        "\n"
      ],
      "metadata": {
        "id": "Ft_qkBbsyxsP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **2.3 Stopword Removal:**\n",
        "\n",
        "Stopwords are common words that do not carry significant meaning and are often removed to focus on the more meaningful words.\n"
      ],
      "metadata": {
        "id": "OPd7u3g4Kff9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **2.4 Lemmatization:**\n",
        "\n",
        "Lemmatization reduces words to their base or root form to unify words with similar meanings."
      ],
      "metadata": {
        "id": "aVSOmM8tKS1B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the WordNetLemmatizer class from the nltk.stem module\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Import the word_tokenize function from the nltk.tokenize module\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Import the Natural Language Toolkit (nltk) library\n",
        "import nltk\n",
        "\n",
        "# Download the WordNet dataset (if not already downloaded)\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Create an instance of the WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Example sentence\n",
        "sentence = \"The quick brown foxes are jumping over the lazy dogs.\"\n",
        "\n",
        "# Tokenize the sentence\n",
        "tokens = word_tokenize(sentence)\n",
        "\n",
        "# Lemmatize the tokens\n",
        "lemmatized_tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "\n",
        "# Print the original tokens and the lemmatized tokens\n",
        "print(\"Original Tokens:\", tokens)\n",
        "print(\"Lemmatized Tokens:\", lemmatized_tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xiFefEwR0WyS",
        "outputId": "5600d36e-8307-4de9-9719-3535faadf3f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Tokens: ['The', 'quick', 'brown', 'foxes', 'are', 'jumping', 'over', 'the', 'laziest', 'dogs', '.']\n",
            "Lemmatized Tokens: ['The', 'quick', 'brown', 'fox', 'are', 'jumping', 'over', 'the', 'laziest', 'dog', '.']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation:**\n",
        "- `lemmatizer.lemmatize(word)`: Lemmatizes each word using the WordNet lemmatizer.\n",
        "\n",
        "**Real-life Example:**  \n",
        "In information extraction, lemmatization can help in grouping different forms of a word (e.g., \"running\" and \"ran\") under a common lemma.\n"
      ],
      "metadata": {
        "id": "WriTVLn51EEb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3. How ML Models Work:**"
      ],
      "metadata": {
        "id": "67TtwEip3y9Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "#### **3.1 Introduction to Machine Learning:**\n",
        "\n",
        "Machine Learning (ML) involves the development of algorithms that enable computers to learn patterns from data and make predictions or decisions without being explicitly programmed. In the context of NLP, machine learning models can be trained on textual data to perform various tasks such as sentiment analysis, classification, or language translation.\n",
        "\n",
        "#### **3.2 Text Representation:**\n",
        "\n",
        "Before feeding text data into machine learning models, it needs to be converted into a numerical format. Several methods are used for text representation, each with its own advantages and drawbacks.\n",
        "\n",
        "#### **3.3 Bag of Words (BoW):**\n",
        "\n",
        "The Bag of Words model represents a document as an unordered set of words, disregarding grammar and word order. It creates a matrix where each row corresponds to a document, and each column corresponds to a unique word in the entire corpus.\n"
      ],
      "metadata": {
        "id": "rAGFmRaa2EoZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming you have already mounted Google Drive and loaded the IMDB dataset into a DataFrame df\n",
        "\n",
        "# Import the CountVectorizer class from the sklearn.feature_extraction.text module\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Create an instance of the CountVectorizer class\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "# Extract the 'review' column from the IMDB dataset as the corpus\n",
        "corpus = df['review'].tolist()\n",
        "\n",
        "# Transform the corpus into a Bag of Words representation\n",
        "X_bow = vectorizer.fit_transform(corpus)\n",
        "\n",
        "# Display the feature names (unique words) in the corpus\n",
        "print(\"Feature Names (Unique Words):\", vectorizer.get_feature_names_out()[:20])  # Displaying the first 20 feature names\n",
        "\n",
        "# Display the Bag of Words matrix (showing only the first 5 reviews for brevity)\n",
        "print(\"Bag of Words Matrix:\")\n",
        "print(X_bow[:5].toarray())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FDTGy_8y2My4",
        "outputId": "a54fe2c0-7b6e-4d69-cfff-84d06bfd2c32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature Names (Unique Words): ['00' '000' '00000000000' '0000000000001' '00000001' '00001' '00015'\n",
            " '000dm' '000s' '001' '003830' '006' '0069' '007' '0079' '007s' '0080'\n",
            " '0083' '009' '0093638']\n",
            "Bag of Words Matrix:\n",
            "[[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation:**\n",
        "- `CountVectorizer`: Converts a collection of text documents to a matrix of token counts.\n",
        "- Each row in the matrix corresponds to a document, and each column corresponds to a unique word in the corpus.\n",
        "- The values in the matrix represent the frequency of each word in the corresponding document.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "X_bRuks_2p5V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **3.4 TF-IDF (Term Frequency-Inverse Document Frequency):** (Skip)\n",
        "\n",
        "TF-IDF is a numerical statistic that reflects how important a word is to a document in a collection or corpus. It considers both the frequency of a word in a document and the overall importance of the word in the entire corpus."
      ],
      "metadata": {
        "id": "77nT458_LO3i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming you have already mounted Google Drive and loaded the IMDB dataset into a DataFrame df\n",
        "\n",
        "# Import the TfidfVectorizer class from the sklearn.feature_extraction.text module\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Take a smaller sample of the 'review' column from the IMDB dataset\n",
        "sample_size = 1000  # You can adjust the sample size based on your needs\n",
        "corpus_sample = df['review'].head(sample_size).tolist()\n",
        "\n",
        "# Create an instance of the TfidfVectorizer class\n",
        "vectorizer_tfidf = TfidfVectorizer()\n",
        "\n",
        "# Create the TF-IDF representation for the given corpus sample\n",
        "X_tfidf = vectorizer_tfidf.fit_transform(corpus_sample)\n",
        "\n",
        "# Display the feature names (unique words) in the corpus\n",
        "print(\"Feature Names (Unique Words):\", vectorizer_tfidf.get_feature_names_out()[:20])  # Displaying the first 20 feature names\n",
        "\n",
        "# Display the TF-IDF matrix (showing only the first 5 reviews for brevity)\n",
        "print(\"TF-IDF Matrix:\")\n",
        "print(X_tfidf[:5].toarray())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6jUazzLR2w1Y",
        "outputId": "8f4df9d9-3773-485f-fcc5-d3169622b37f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature Names (Unique Words): ['00' '000' '007' '00am' '01pm' '08' '10' '100' '1000' '100th' '101' '102'\n",
            " '103' '105' '11' '12' '120' '13' '135' '13th']\n",
            "TF-IDF Matrix:\n",
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation:**\n",
        "- `TfidfVectorizer`: Converts a collection of raw documents to a matrix of TF-IDF features.\n",
        "- The TF-IDF matrix is similar to the Bag of Words matrix but gives more weight to terms that are important to a specific document.\n",
        "\n"
      ],
      "metadata": {
        "id": "84FgH8AB3Hl2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **3.5 Word Embeddings (Word2Vec):**\n",
        "\n",
        "Word Embeddings represent words as dense vectors in a continuous vector space, capturing semantic relationships between words. Word2Vec is a popular method for generating word embeddings.\n"
      ],
      "metadata": {
        "id": "bPaBBGcVLSkF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![picture](https://drive.google.com/uc?export=view&id=1n0mUZcYsRLo_pInU2xiUG--t5bOISc0Q)\n",
        "\n"
      ],
      "metadata": {
        "id": "q9bjj-Y96kdk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming you have already mounted Google Drive and loaded the IMDB dataset into a DataFrame df\n",
        "\n",
        "# Import the Word2Vec class from the gensim.models module\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "# Import the word_tokenize function from the nltk.tokenize module\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Tokenize a smaller sample of the 'review' column from the IMDB dataset\n",
        "sample_size = 1000  # You can adjust the sample size based on your needs\n",
        "tokenized_corpus_sample = [word_tokenize(review.lower()) for review in df['review'].head(sample_size).tolist()]\n",
        "\n",
        "# Create the Word2Vec model\n",
        "word2vec_model = Word2Vec(sentences=tokenized_corpus_sample, vector_size=100, window=5, min_count=1, workers=4)\n",
        "# Adjust the parameters (e.g., vector_size, window) based on your specific requirements.\n",
        "\n",
        "# Get the vector representation of a word (e.g., 'document')\n",
        "vector_representation = word2vec_model.wv['document']\n",
        "\n",
        "# Print the vector representation\n",
        "print(\"Vector representation of 'document':\", vector_representation)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kQRkEPhw3LcY",
        "outputId": "7f0b2b09-d03a-4d78-8650-cc1943aab110"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vector representation of 'document': [-0.01799615  0.01300799  0.00495014 -0.00586682 -0.00115296 -0.0326869\n",
            "  0.00814584  0.06054067 -0.02035532 -0.02982742  0.00301758 -0.04267221\n",
            "  0.00667032  0.01732409  0.00645217 -0.03111579  0.01347287 -0.03030831\n",
            " -0.01232108 -0.03072064  0.00579258  0.01554295  0.02778946  0.00035944\n",
            " -0.00729582  0.00198373 -0.03189528  0.00383202 -0.02696364  0.00714664\n",
            "  0.02200642 -0.01503243 -0.00250842 -0.01869168 -0.02314418  0.01579471\n",
            "  0.01724296 -0.01859335  0.00939682 -0.02926154  0.01487183 -0.01600094\n",
            " -0.02168584 -0.01009515  0.00540448 -0.01167929 -0.0138716   0.00317287\n",
            "  0.00301045  0.00172426  0.01636955 -0.03009641  0.01105109  0.00368481\n",
            " -0.00448172  0.00808757  0.01398096 -0.0123611  -0.02044742  0.00928421\n",
            "  0.00129415 -0.01355246  0.00084942 -0.00437145 -0.01513183  0.01167085\n",
            " -0.01791506  0.01776645 -0.01820335  0.01870342 -0.00245577  0.01904474\n",
            "  0.02756433  0.00783187  0.02264715  0.00305092 -0.00458813 -0.00446247\n",
            " -0.02884027 -0.004688   -0.02920558  0.01047619 -0.01444308  0.03790974\n",
            " -0.00472834  0.01185873  0.00822893  0.00959576  0.02724676  0.01732514\n",
            "  0.02688865  0.01554518 -0.00072193  0.00042166  0.05949353  0.02476264\n",
            " -0.00882263 -0.0216234   0.00261386 -0.00713954]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Explanation:**\n",
        "- `Word2Vec`: Learns word embeddings from a large corpus of text.\n",
        "- Each word is represented as a dense vector, capturing semantic relationships."
      ],
      "metadata": {
        "id": "UptG8x5Q3q1e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4. Word Embeddings (Word2Vec):**"
      ],
      "metadata": {
        "id": "aKkuZqdt5cC0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **4.1 Word2Vec Basics:**\n",
        "\n",
        "Word2Vec is a popular word embedding technique that represents words as dense vectors in a continuous vector space. It captures semantic relationships between words and is widely used in natural language processing tasks.\n",
        "\n",
        "Word2Vec has two primary architectures:\n",
        "\n",
        "1. **Continuous Bag of Words (CBOW):**\n",
        "   - Predicts the current word given its context words.\n",
        "   - Suitable for smaller datasets.\n",
        "\n",
        "2. **Skip-gram:**\n",
        "   - Predicts context words given the current word.\n",
        "   - Performs well with larger datasets and captures more detailed word relationships.\n",
        "\n",
        "#### **4.2 Gensim Library for Word2Vec:**\n",
        "\n",
        "Gensim is a Python library that provides tools for working with Word2Vec models. It's efficient and allows for the training of Word2Vec models on large datasets.\n",
        "\n",
        "#### **Real-life Code Example:**\n",
        "\n",
        "Let's demonstrate how to use Gensim to train a Word2Vec model on a small corpus."
      ],
      "metadata": {
        "id": "uxslt7cN5gfq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming you have already mounted Google Drive and loaded the IMDB dataset into a DataFrame df\n",
        "\n",
        "# Import the Word2Vec class from the gensim.models module\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "# Import the word_tokenize function from the nltk.tokenize module\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Import the Natural Language Toolkit (nltk) library\n",
        "import nltk\n",
        "# Download the Punkt tokenizer model (if not already downloaded)\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Take a smaller sample of the 'review' column from the IMDB dataset\n",
        "sample_size = 1000  # You can adjust the sample size based on your needs\n",
        "tokenized_corpus_sample = [word_tokenize(review.lower()) for review in df['review'].head(sample_size).tolist()]\n",
        "\n",
        "# Create the Word2Vec model (Skip-gram)\n",
        "word2vec_model = Word2Vec(sentences=tokenized_corpus_sample, vector_size=100, window=5, min_count=1, workers=4, sg=1)\n",
        "# Adjust the parameters (e.g., vector_size, window) based on your specific requirements.\n",
        "\n",
        "# Save the Word2Vec model\n",
        "word2vec_model.save(\"/content/drive/MyDrive/Master Class/word2vec_model_imdb_sample.model\")\n",
        "# - Saves the trained Word2Vec model to a file named \"word2vec_model_imdb_sample.model\".\n",
        "\n",
        "# Load the Word2Vec model\n",
        "loaded_model = Word2Vec.load(\"/content/drive/MyDrive/Master Class/word2vec_model_imdb_sample.model\")\n",
        "# - Loads the Word2Vec model from the saved file.\n",
        "\n",
        "# Get the vector representation of a word (e.g., 'movie')\n",
        "vector_representation = loaded_model.wv['movie']\n",
        "# - loaded_model.wv['movie']: Retrieves the vector representation of the word 'movie' from the loaded Word2Vec model.\n",
        "\n",
        "# Print the vector representation\n",
        "print(\"Vector representation of 'movie':\", vector_representation)\n",
        "# - Prints the vector representation of the word 'movie'.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Zz8ShPW5uZ9",
        "outputId": "931348ce-4791-4244-bd16-08a9c92f5808"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vector representation of 'movie': [-0.04137838  0.45587063  0.43418795  0.42026514 -0.15631789 -0.2346355\n",
            " -0.01740664  0.75538415 -0.39739498 -0.19003744  0.09647486 -0.301931\n",
            "  0.04421014  0.13624126  0.16692582 -0.22978473  0.20822304 -0.23731\n",
            " -0.11544676 -1.037378   -0.06992677  0.20673127  0.51394564 -0.5281713\n",
            " -0.05746479  0.25768358 -0.7350139   0.31799152  0.09495896  0.06307528\n",
            "  0.2668496  -0.04243638  0.2659982  -0.38196835 -0.23148724  0.08468099\n",
            "  0.6383195  -0.22720338 -0.15904032 -0.700673   -0.13491616 -0.21236315\n",
            " -0.24900283  0.12050357 -0.1567658  -0.45392305 -0.14960532  0.1221492\n",
            " -0.04226052  0.18164657 -0.14724125 -0.11589345 -0.4547419  -0.12299178\n",
            "  0.19331095 -0.2721154  -0.0184927  -0.38386193 -0.17120913  0.3001281\n",
            "  0.24102426  0.41272256 -0.00529413  0.08416962 -0.48258567  0.5757731\n",
            "  0.08983218  0.3334069  -0.43658233  0.3951002  -0.26522332  0.40785015\n",
            "  0.02912864 -0.16939105  0.48818204  0.11548709  0.0523996   0.12026362\n",
            " -0.22329101 -0.0388239  -0.10945214  0.08658427  0.4064346   0.5091647\n",
            " -0.4943685   0.15746301  0.29060325 -0.11514846  0.16089015  0.48545918\n",
            "  0.36556464 -0.11995062  0.07261783  0.35260385  0.9518649  -0.04965856\n",
            " -0.16999389 -0.05104041 -0.09494426  0.11118979]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation:**\n",
        "- `Word2Vec`: Initializes a Word2Vec model.\n",
        "- `sentences`: The tokenized sentences from the corpus.\n",
        "- `vector_size`: Dimensionality of the word vectors.\n",
        "- `window`: Maximum distance between the current and predicted word within a sentence.\n",
        "- `min_count`: Ignores all words with a total frequency lower than this.\n",
        "- `workers`: Number of CPU cores to use when training the model.\n",
        "- `sg=1`: Indicates the Skip-gram architecture.\n",
        "\n",
        "In the example, we tokenize the corpus, train a Word2Vec model using Gensim, save the model, and then load it back to obtain the vector representation of the word 'word'.\n",
        "\n",
        "This Word2Vec model can be used to obtain vector representations for words in a given context, enabling semantic similarity and analogy calculations."
      ],
      "metadata": {
        "id": "HTuGU5PB59HG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **5. Seq2Seq Models:**"
      ],
      "metadata": {
        "id": "90ZWvBaq6Wjd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **5.1 Introduction to Seq2Seq:**\n",
        "\n",
        "Sequence-to-Sequence (Seq2Seq) models are a type of neural network architecture designed for tasks involving sequences, such as language translation, summarization, and chatbot responses. The basic idea is to use two recurrent neural networks (RNNs) known as an encoder and a decoder to transform input sequences into output sequences.\n",
        "\n",
        "#### **5.2 Encoder-Decoder Architecture:**\n",
        "\n",
        "The Seq2Seq model comprises two main components: an encoder and a decoder.\n",
        "\n",
        "**5.2.1 Encoder:**\n",
        "   - Takes an input sequence and encodes it into a fixed-size context vector.\n",
        "   - The context vector contains essential information about the input sequence.\n",
        "   - Commonly implemented using recurrent neural networks (RNNs) or long short-term memory networks (LSTMs).\n",
        "\n",
        "**5.2.2  Decoder:**\n",
        "   - Takes the context vector produced by the encoder and generates the output sequence.\n",
        "   - Predicts one element at a time, often autoregressively.\n",
        "   - Also commonly implemented using RNNs or LSTMs.\n",
        "\n",
        "#### **Real-life Code Example:**\n",
        "\n",
        "Let's create a simple Seq2Seq model using the Keras library for language translation. In this example, we'll train a Seq2Seq model to translate English sentences to French."
      ],
      "metadata": {
        "id": "CCp23jit7YTE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Flatten\n",
        "from tensorflow.keras.datasets import imdb\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Load the IMDB dataset\n",
        "vocab_size = 10000  # Consider the top 10,000 words\n",
        "max_len = 100  # Consider the first 100 words of each review\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=vocab_size)\n",
        "x_train = pad_sequences(x_train, maxlen=max_len)\n",
        "x_test = pad_sequences(x_test, maxlen=max_len)\n",
        "\n",
        "# Build the Seq2Seq model for sentiment analysis\n",
        "latent_dim = 4\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=vocab_size, output_dim=latent_dim, input_length=max_len))\n",
        "model.add(LSTM(latent_dim))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model using binary crossentropy loss and the Adam optimizer.\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model using the IMDB training data.\n",
        "model.fit(x_train, y_train, epochs=5, batch_size=32, validation_split=0.2)\n",
        "\n",
        "# Evaluate the model on the IMDB test data.\n",
        "loss, accuracy = model.evaluate(x_test, y_test)\n",
        "print(f\"Test Loss: {loss:.4f}, Test Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9p29jHXv7ghP",
        "outputId": "478e4066-d263-47ac-caa0-77c63eb71693"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "625/625 [==============================] - 22s 32ms/step - loss: 0.5702 - accuracy: 0.7165 - val_loss: 0.4554 - val_accuracy: 0.8122\n",
            "Epoch 2/5\n",
            "625/625 [==============================] - 19s 31ms/step - loss: 0.3723 - accuracy: 0.8521 - val_loss: 0.3913 - val_accuracy: 0.8320\n",
            "Epoch 3/5\n",
            "625/625 [==============================] - 19s 30ms/step - loss: 0.2827 - accuracy: 0.8938 - val_loss: 0.3841 - val_accuracy: 0.8308\n",
            "Epoch 4/5\n",
            "625/625 [==============================] - 30s 48ms/step - loss: 0.2296 - accuracy: 0.9191 - val_loss: 0.3765 - val_accuracy: 0.8398\n",
            "Epoch 5/5\n",
            "625/625 [==============================] - 28s 45ms/step - loss: 0.1917 - accuracy: 0.9342 - val_loss: 0.4141 - val_accuracy: 0.8326\n",
            "782/782 [==============================] - 6s 8ms/step - loss: 0.4184 - accuracy: 0.8350\n",
            "Test Loss: 0.4184, Test Accuracy: 0.8350\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation:**\n",
        "\n",
        "This code demonstrates a simple Seq2Seq model using Keras for English-to-French word translation:\n",
        "\n",
        "1. **Data Preparation:**\n",
        "   - Lists of English and French words.\n",
        "   - Tokenization using dictionaries (`tokenizer_eng` and `tokenizer_frn`).\n",
        "\n",
        "2. **Sequence and One-Hot Encoding:**\n",
        "   - Conversion of words to integer sequences (`seqs_eng` and `seqs_frn`).\n",
        "   - One-hot encoding of sequences (`one_hot_eng` and `one_hot_frn`).\n",
        "\n",
        "3. **Seq2Seq Model:**\n",
        "   - Sequential model with Embedding, LSTM, and Dense layers.\n",
        "   - Embedding layer maps input sequences to vectors.\n",
        "   - LSTM layer captures sequential patterns.\n",
        "   - Dense layer with softmax activation predicts the output sequence.\n",
        "\n",
        "4. **Model Compilation and Training:**\n",
        "   - Compilation with Adam optimizer, categorical cross-entropy loss, and accuracy metric.\n",
        "   - Training on one-hot encoded English and French sequences.\n",
        "\n",
        "5. **Inference:**\n",
        "   - Input word 'hello' is converted to a one-hot encoded sequence.\n",
        "   - Model predicts the output sequence.\n",
        "   - Predicted word is obtained by finding the word with the highest probability in the French vocabulary.\n",
        "\n",
        "6. **Results Printing:**\n",
        "   - Prints the input English word and the predicted French word.\n",
        "\n",
        "This example is educational, illustrating the fundamental structure of a Seq2Seq model for word translation."
      ],
      "metadata": {
        "id": "sol8q086-Pz1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **6. Transformer Models with Attention**"
      ],
      "metadata": {
        "id": "9ZFVAtD8-l6f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **6.1 Introduction to Transformers:**\n",
        "\n",
        "- Transformers are a type of neural network architecture introduced in the paper \"Attention is All You Need.\"\n",
        "- Key components: self-attention mechanism, encoder-decoder structure.\n",
        "- Effective in capturing long-range dependencies in sequences.\n"
      ],
      "metadata": {
        "id": "NjgJWjQo_yFr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the transformers library\n",
        "!pip install transformers\n",
        "\n",
        "# Import necessary classes from the transformers library\n",
        "from transformers import BertTokenizer, BertModel\n",
        "\n",
        "# Tokenize input text\n",
        "# Instantiate a BERT tokenizer for the 'bert-base-uncased' model\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Input text to be tokenized\n",
        "text = \"This is an introduction to transformers.\"\n",
        "\n",
        "# Encode the input text using the BERT tokenizer\n",
        "# 'return_tensors' parameter specifies that PyTorch tensors should be returned\n",
        "encoded_tokens = tokenizer.encode(text, return_tensors='pt')\n",
        "\n",
        "# Load pre-trained BERT model\n",
        "# Instantiate a BERT model for the 'bert-base-uncased' variant\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Forward pass through the BERT model\n",
        "# Pass the encoded tokens through the BERT model to obtain the output\n",
        "output = model(encoded_tokens)\n",
        "\n",
        "# Print the output\n",
        "print(output)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J3wSg7jgAQLV",
        "outputId": "aeabd6f9-1bbf-43ba-a898-4c89b9ab2e55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.11.17)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[-0.3407, -0.0158, -0.1392,  ..., -0.4034,  0.3556,  0.6352],\n",
            "         [-0.7462, -0.6976, -0.1616,  ..., -0.0087,  0.8424,  0.2508],\n",
            "         [-0.5540, -0.6966,  0.8854,  ...,  0.3237,  0.1576,  0.9480],\n",
            "         ...,\n",
            "         [ 2.2967, -0.1664,  0.3546,  ..., -0.5724, -0.3273,  0.3215],\n",
            "         [ 0.5750,  0.0995, -0.3975,  ...,  0.2667, -0.2205, -0.4782],\n",
            "         [ 0.5753,  0.1630, -0.0906,  ...,  0.2518, -0.5264, -0.3900]]],\n",
            "       grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[-0.9570, -0.5569, -0.9623,  0.9004,  0.7563, -0.2063,  0.9565,  0.4562,\n",
            "         -0.8887, -1.0000, -0.6778,  0.9670,  0.9836,  0.6399,  0.9413, -0.8410,\n",
            "         -0.3558, -0.6316,  0.3938, -0.7038,  0.6955,  1.0000,  0.0240,  0.4847,\n",
            "          0.5733,  0.9959, -0.8466,  0.9564,  0.9773,  0.7756, -0.8173,  0.3507,\n",
            "         -0.9875, -0.3182, -0.9632, -0.9967,  0.6160, -0.8233, -0.0236, -0.1948,\n",
            "         -0.9170,  0.4625,  1.0000,  0.0652,  0.5830, -0.3614, -1.0000,  0.2629,\n",
            "         -0.9260,  0.9676,  0.9347,  0.9498,  0.3067,  0.6403,  0.6274, -0.3597,\n",
            "          0.0159,  0.2364, -0.3216, -0.6771, -0.6901,  0.4505, -0.9117, -0.9510,\n",
            "          0.9470,  0.8913, -0.2293, -0.4070, -0.2071,  0.0100,  0.9471,  0.2995,\n",
            "         -0.4960, -0.8558,  0.8346,  0.3286, -0.7348,  1.0000, -0.7201, -0.9817,\n",
            "          0.8907,  0.8838,  0.6860, -0.5468,  0.6916, -1.0000,  0.7070, -0.2436,\n",
            "         -0.9895,  0.2960,  0.7271, -0.3202,  0.7565,  0.7211, -0.6350, -0.6644,\n",
            "         -0.5076, -0.9237, -0.3867, -0.4914,  0.1802, -0.3667, -0.5064, -0.5494,\n",
            "          0.4355, -0.6083, -0.6374,  0.7727,  0.1704,  0.7996,  0.5461, -0.5209,\n",
            "          0.5600, -0.9748,  0.7727, -0.4947, -0.9890, -0.7150, -0.9874,  0.8041,\n",
            "         -0.4165, -0.2706,  0.9717, -0.4591,  0.5207, -0.2485, -0.9669, -1.0000,\n",
            "         -0.7675, -0.7458, -0.3924, -0.4527, -0.9790, -0.9663,  0.7324,  0.9738,\n",
            "          0.3809,  1.0000, -0.4258,  0.9590, -0.5306, -0.7046,  0.5845, -0.5587,\n",
            "          0.8837,  0.5810, -0.8018,  0.3213, -0.4796,  0.6069, -0.8701, -0.4443,\n",
            "         -0.8889, -0.9412, -0.4439,  0.9540, -0.5618, -0.9693, -0.1039, -0.4094,\n",
            "         -0.5693,  0.8993,  0.8053,  0.4782, -0.4452,  0.5710,  0.5594,  0.6917,\n",
            "         -0.9301, -0.2923,  0.5070, -0.4356, -0.9463, -0.9843, -0.5219,  0.7752,\n",
            "          0.9909,  0.8519,  0.4075,  0.9149, -0.4059,  0.8122, -0.9609,  0.9834,\n",
            "         -0.2935,  0.3783, -0.6308,  0.6630, -0.9257,  0.1693,  0.9283, -0.7554,\n",
            "         -0.9144, -0.2328, -0.5437, -0.5167, -0.8709,  0.6766, -0.4026, -0.4589,\n",
            "         -0.2554,  0.9377,  0.9928,  0.8397,  0.4167,  0.7289, -0.9255, -0.6246,\n",
            "          0.1903,  0.3244,  0.1990,  0.9951, -0.7631, -0.3504, -0.9477, -0.9875,\n",
            "         -0.0090, -0.9291, -0.3032, -0.8132,  0.7969, -0.3735,  0.6983,  0.5421,\n",
            "         -0.9955, -0.8489,  0.4647, -0.5909,  0.6134, -0.2687,  0.7745,  0.9794,\n",
            "         -0.7772,  0.7467,  0.9308, -0.9443, -0.7930,  0.9042, -0.4725,  0.9466,\n",
            "         -0.7555,  0.9976,  0.9618,  0.8566, -0.9441, -0.8625, -0.9390, -0.8189,\n",
            "         -0.2121,  0.3368,  0.9464,  0.7716,  0.5195,  0.2753, -0.7632,  0.9993,\n",
            "         -0.7382, -0.9560, -0.4764, -0.4576, -0.9889,  0.9412,  0.3125,  0.5426,\n",
            "         -0.6607, -0.8298, -0.9675,  0.9516,  0.2144,  0.9970, -0.4253, -0.9674,\n",
            "         -0.7389, -0.9257, -0.0172, -0.4295, -0.5491,  0.0229, -0.9574,  0.6046,\n",
            "          0.7187,  0.6281, -0.9240,  0.9996,  1.0000,  0.9826,  0.9221,  0.9560,\n",
            "         -1.0000, -0.5782,  1.0000, -0.9970, -1.0000, -0.9362, -0.7347,  0.4894,\n",
            "         -1.0000, -0.2298, -0.1089, -0.9404,  0.7889,  0.9832,  0.9976, -1.0000,\n",
            "          0.9357,  0.9620, -0.7818,  0.9769, -0.5695,  0.9777,  0.6611,  0.5444,\n",
            "         -0.3435,  0.5613, -0.9612, -0.9302, -0.7679, -0.7913,  0.9997,  0.2083,\n",
            "         -0.8920, -0.9338,  0.6489, -0.1691,  0.0411, -0.9706, -0.2512,  0.7219,\n",
            "          0.8732,  0.2981,  0.5089, -0.8527,  0.4659,  0.2410,  0.5625,  0.7682,\n",
            "         -0.9579, -0.7049, -0.4721,  0.1297, -0.7078, -0.9693,  0.9754, -0.5790,\n",
            "          0.9563,  1.0000,  0.5487, -0.9222,  0.7480,  0.4062, -0.6651,  1.0000,\n",
            "          0.8766, -0.9829, -0.6658,  0.7853, -0.6704, -0.7638,  0.9999, -0.3847,\n",
            "         -0.8135, -0.5791,  0.9764, -0.9904,  0.9993, -0.9520, -0.9702,  0.9708,\n",
            "          0.9506, -0.7425, -0.8622,  0.2401, -0.8488,  0.4200, -0.9689,  0.8525,\n",
            "          0.6218, -0.1751,  0.8812, -0.9431, -0.7020,  0.5294, -0.7558, -0.3892,\n",
            "          0.9826,  0.6426, -0.3916,  0.1147, -0.3268, -0.6037, -0.9764,  0.6467,\n",
            "          1.0000, -0.4878,  0.8614, -0.6018, -0.1591, -0.0434,  0.6791,  0.7169,\n",
            "         -0.4355, -0.9381,  0.9108, -0.9816, -0.9864,  0.8292,  0.3010, -0.4227,\n",
            "          1.0000,  0.6608,  0.3392,  0.5734,  0.9963,  0.0486,  0.7261,  0.9376,\n",
            "          0.9822, -0.3275,  0.7221,  0.9199, -0.9582, -0.3763, -0.7606,  0.0904,\n",
            "         -0.9304, -0.0998, -0.9581,  0.9711,  0.9763,  0.5463,  0.3809,  0.8255,\n",
            "          1.0000, -0.6335,  0.7473, -0.4008,  0.9069, -1.0000, -0.9303, -0.5256,\n",
            "         -0.1688, -0.9050, -0.4699,  0.4240, -0.9776,  0.9139,  0.8355, -0.9938,\n",
            "         -0.9917, -0.4981,  0.9195,  0.0766, -0.9952, -0.7957, -0.7159,  0.7536,\n",
            "         -0.4456, -0.9455, -0.4594, -0.4367,  0.6429, -0.3873,  0.7043,  0.9162,\n",
            "          0.6260, -0.8699, -0.4933, -0.1328, -0.8602,  0.8826, -0.8886, -0.9624,\n",
            "         -0.2451,  1.0000, -0.6926,  0.9329,  0.8752,  0.8002, -0.3885,  0.2724,\n",
            "          0.9721,  0.3219, -0.8395, -0.9360, -0.6831, -0.5295,  0.7524,  0.6956,\n",
            "          0.8338,  0.8250,  0.8678,  0.2350, -0.2113,  0.1267,  0.9999, -0.1999,\n",
            "         -0.3723, -0.6717, -0.2008, -0.3905, -0.3684,  1.0000,  0.4381,  0.5849,\n",
            "         -0.9906, -0.9364, -0.9732,  1.0000,  0.8429, -0.9160,  0.8432,  0.7200,\n",
            "         -0.2821,  0.8707, -0.3050, -0.3854,  0.3840,  0.1985,  0.9637, -0.6952,\n",
            "         -0.9719, -0.7195,  0.5820, -0.9695,  1.0000, -0.7292, -0.3995, -0.4705,\n",
            "         -0.3553,  0.3737,  0.0894, -0.9832, -0.4396,  0.2703,  0.9649,  0.3830,\n",
            "         -0.7235, -0.9374,  0.9317,  0.8414, -0.9530, -0.9532,  0.9682, -0.9811,\n",
            "          0.7447,  1.0000,  0.5167,  0.4534,  0.3964, -0.5920,  0.5030, -0.4336,\n",
            "          0.8061, -0.9655, -0.4400, -0.3460,  0.4186, -0.3089, -0.4498,  0.7969,\n",
            "          0.3226, -0.6655, -0.7259, -0.2982,  0.5725,  0.9280, -0.3720, -0.3185,\n",
            "          0.2239, -0.2524, -0.9471, -0.4857, -0.6144, -1.0000,  0.7961, -1.0000,\n",
            "          0.6960,  0.3857, -0.1803,  0.8648,  0.5120,  0.8334, -0.8116, -0.9217,\n",
            "          0.1930,  0.8612, -0.3990, -0.6910, -0.7784,  0.5060, -0.3247,  0.3313,\n",
            "         -0.7194,  0.8379, -0.3427,  1.0000,  0.2523, -0.7779, -0.9903,  0.3698,\n",
            "         -0.3737,  1.0000, -0.9510, -0.9546,  0.5192, -0.8779, -0.8675,  0.4270,\n",
            "          0.0106, -0.8800, -0.9791,  0.9748,  0.9196, -0.6902,  0.5906, -0.4959,\n",
            "         -0.6962,  0.1340,  0.9417,  0.9863,  0.6988,  0.9497,  0.0671, -0.3833,\n",
            "          0.9770,  0.3720,  0.6185,  0.2531,  1.0000,  0.4946, -0.9483, -0.1239,\n",
            "         -0.9886, -0.3656, -0.9660,  0.3974,  0.3456,  0.9352, -0.4123,  0.9754,\n",
            "         -0.9345,  0.1383, -0.7367, -0.6360,  0.4395, -0.9495, -0.9878, -0.9866,\n",
            "          0.8307, -0.5486, -0.0434,  0.3280,  0.2056,  0.6079,  0.5783, -1.0000,\n",
            "          0.9436,  0.5529,  0.9520,  0.9590,  0.7943,  0.7042,  0.4266, -0.9879,\n",
            "         -0.9900, -0.5168, -0.3164,  0.8438,  0.7409,  0.9215,  0.5480, -0.5926,\n",
            "         -0.5871, -0.6967, -0.6665, -0.9932,  0.5256, -0.8002, -0.9799,  0.9623,\n",
            "          0.1446, -0.1999, -0.2471, -0.9122,  0.9724,  0.9119,  0.5136,  0.1714,\n",
            "          0.6120,  0.9092,  0.9796,  0.9878, -0.9172,  0.8927, -0.8189,  0.6572,\n",
            "          0.7640, -0.9532,  0.2997,  0.6139, -0.5657,  0.4233, -0.4614, -0.9885,\n",
            "          0.7259, -0.4464,  0.7330, -0.5983, -0.0309, -0.5718, -0.3208, -0.8599,\n",
            "         -0.8235,  0.7294,  0.5480,  0.9171,  0.9015, -0.1725, -0.8558, -0.3577,\n",
            "         -0.8498, -0.9428,  0.9700, -0.2408, -0.5125,  0.7981,  0.0571,  0.8952,\n",
            "          0.4456, -0.5547, -0.4358, -0.8524,  0.9332, -0.6069, -0.7022, -0.7286,\n",
            "          0.8083,  0.4664,  1.0000, -0.8459, -0.9606, -0.5368, -0.5426,  0.4746,\n",
            "         -0.5777, -1.0000,  0.5023, -0.7426,  0.8694, -0.8247,  0.9126, -0.7624,\n",
            "         -0.9912, -0.3989,  0.6658,  0.8444, -0.5859, -0.7850,  0.7099, -0.6154,\n",
            "          0.9922,  0.9028, -0.6464, -0.0828,  0.7403, -0.9046, -0.7554,  0.9260]],\n",
            "       grad_fn=<TanhBackward0>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **6.2 Self-Attention Mechanism:**\n",
        "#### Theory:\n",
        "- Self-attention allows a model to weigh the importance of different words in a sequence relative to each other.\n",
        "- Attention scores are computed based on the similarity between words.\n",
        "\n",
        "#### Code Example:"
      ],
      "metadata": {
        "id": "t2ndmUd8A2di"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import pandas as pd\n",
        "\n",
        "# Load your IMDB dataset\n",
        "# Replace 'your_imdb_dataset.csv' with the actual path to your dataset\n",
        "imdb_df = pd.read_csv('/content/drive/MyDrive/Master Class/IMDB Dataset.csv')\n",
        "\n",
        "# Choose a smaller subset of the dataset for demonstration purposes\n",
        "subset_size = 5\n",
        "imdb_subset = imdb_df.head(subset_size)\n",
        "\n",
        "# Assume 'review' column contains preprocessed text data\n",
        "# Convert the text data to tensors (you might want to use embeddings in a real-world scenario)\n",
        "text_tensors = torch.rand((subset_size, 10, 10))  # Replace with your actual text to tensor conversion\n",
        "\n",
        "# Assume query, key, and value are derived from the text data (in practice, they might be learned parameters)\n",
        "query = text_tensors\n",
        "key = text_tensors\n",
        "value = text_tensors\n",
        "\n",
        "# Compute attention scores using scaled dot-product attention\n",
        "attention_scores = F.softmax(torch.bmm(query, key.transpose(1, 2)), dim=-1)\n",
        "\n",
        "# Compute the weighted sum using the attention scores and values\n",
        "weighted_sum = torch.bmm(attention_scores, value)\n",
        "\n",
        "# Print the result\n",
        "print(weighted_sum)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1U0zr8PqBLzB",
        "outputId": "726c3ce5-58f0-4068-daa4-b291b67e9809"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[0.5341, 0.5769, 0.6861, 0.7477, 0.5404, 0.3739, 0.5394, 0.5204,\n",
            "          0.3726, 0.7516],\n",
            "         [0.4455, 0.5178, 0.6766, 0.7877, 0.5551, 0.4618, 0.5949, 0.5512,\n",
            "          0.3458, 0.7800],\n",
            "         [0.6210, 0.6750, 0.6444, 0.7448, 0.5077, 0.3322, 0.5309, 0.5663,\n",
            "          0.3179, 0.7416],\n",
            "         [0.5772, 0.5678, 0.6786, 0.6870, 0.4948, 0.3471, 0.5189, 0.4879,\n",
            "          0.3910, 0.6862],\n",
            "         [0.5964, 0.6513, 0.6520, 0.7353, 0.5267, 0.3217, 0.5309, 0.5006,\n",
            "          0.3443, 0.7222],\n",
            "         [0.5157, 0.5976, 0.6542, 0.7731, 0.4915, 0.4169, 0.5883, 0.5433,\n",
            "          0.3230, 0.7291],\n",
            "         [0.6740, 0.7218, 0.5578, 0.7275, 0.5761, 0.2513, 0.4591, 0.4606,\n",
            "          0.3518, 0.6907],\n",
            "         [0.6107, 0.6337, 0.6357, 0.7171, 0.4793, 0.3154, 0.5130, 0.4887,\n",
            "          0.3629, 0.6683],\n",
            "         [0.6296, 0.6070, 0.6628, 0.6666, 0.4919, 0.2933, 0.4793, 0.4677,\n",
            "          0.3995, 0.6661],\n",
            "         [0.6378, 0.6804, 0.5877, 0.7156, 0.4859, 0.2871, 0.4814, 0.4609,\n",
            "          0.3491, 0.6414]],\n",
            "\n",
            "        [[0.7066, 0.6296, 0.7301, 0.6115, 0.4780, 0.6710, 0.3617, 0.7580,\n",
            "          0.4033, 0.4682],\n",
            "         [0.7710, 0.6232, 0.7517, 0.5231, 0.4586, 0.5563, 0.4577, 0.7352,\n",
            "          0.5598, 0.3917],\n",
            "         [0.7353, 0.6137, 0.7338, 0.6308, 0.4499, 0.6584, 0.4395, 0.6552,\n",
            "          0.4225, 0.4528],\n",
            "         [0.7363, 0.6493, 0.7341, 0.5702, 0.4712, 0.6231, 0.4269, 0.7264,\n",
            "          0.4760, 0.4243],\n",
            "         [0.7566, 0.6663, 0.7374, 0.5768, 0.4872, 0.6025, 0.4502, 0.7648,\n",
            "          0.5024, 0.4251],\n",
            "         [0.7090, 0.6621, 0.7228, 0.5793, 0.4813, 0.6366, 0.4137, 0.7239,\n",
            "          0.4326, 0.4447],\n",
            "         [0.7348, 0.6285, 0.7375, 0.5621, 0.4469, 0.6314, 0.4135, 0.6804,\n",
            "          0.4748, 0.4142],\n",
            "         [0.7252, 0.6577, 0.7117, 0.5287, 0.4899, 0.5683, 0.4198, 0.7311,\n",
            "          0.4988, 0.4296],\n",
            "         [0.7731, 0.6474, 0.7723, 0.6363, 0.5031, 0.6500, 0.3632, 0.7895,\n",
            "          0.4106, 0.4555],\n",
            "         [0.7570, 0.7198, 0.7242, 0.6270, 0.5064, 0.6229, 0.5451, 0.7838,\n",
            "          0.5062, 0.4335]],\n",
            "\n",
            "        [[0.4940, 0.4602, 0.6708, 0.4438, 0.7375, 0.6020, 0.4345, 0.4697,\n",
            "          0.6594, 0.5091],\n",
            "         [0.3651, 0.5661, 0.6790, 0.3035, 0.7226, 0.5945, 0.5698, 0.6121,\n",
            "          0.7497, 0.6523],\n",
            "         [0.4114, 0.6201, 0.5623, 0.3158, 0.7458, 0.4202, 0.5421, 0.5446,\n",
            "          0.6107, 0.6128],\n",
            "         [0.6141, 0.6531, 0.5868, 0.3821, 0.6615, 0.4322, 0.4396, 0.5390,\n",
            "          0.6580, 0.5926],\n",
            "         [0.6006, 0.4550, 0.7231, 0.4407, 0.7177, 0.6231, 0.3916, 0.4041,\n",
            "          0.6806, 0.5039],\n",
            "         [0.5128, 0.5683, 0.6084, 0.3679, 0.7338, 0.4907, 0.4427, 0.5288,\n",
            "          0.6404, 0.5946],\n",
            "         [0.5260, 0.4699, 0.6214, 0.4398, 0.6383, 0.5913, 0.4596, 0.4672,\n",
            "          0.6508, 0.4882],\n",
            "         [0.6354, 0.5986, 0.6338, 0.3980, 0.6548, 0.4905, 0.4161, 0.4404,\n",
            "          0.6581, 0.5543],\n",
            "         [0.6094, 0.4654, 0.7170, 0.4228, 0.6425, 0.6484, 0.4418, 0.4130,\n",
            "          0.7037, 0.5025],\n",
            "         [0.4650, 0.5784, 0.6399, 0.3587, 0.7384, 0.4739, 0.4597, 0.4774,\n",
            "          0.6595, 0.5934]],\n",
            "\n",
            "        [[0.4508, 0.6082, 0.5540, 0.6339, 0.5912, 0.6178, 0.5189, 0.5443,\n",
            "          0.3125, 0.5658],\n",
            "         [0.5362, 0.7204, 0.4995, 0.6358, 0.5676, 0.6457, 0.4660, 0.5119,\n",
            "          0.3619, 0.5491],\n",
            "         [0.5034, 0.7517, 0.5368, 0.5999, 0.5602, 0.6492, 0.4330, 0.4157,\n",
            "          0.4607, 0.5573],\n",
            "         [0.5564, 0.6209, 0.5448, 0.5115, 0.4441, 0.5911, 0.4042, 0.4722,\n",
            "          0.4414, 0.5992],\n",
            "         [0.5872, 0.6863, 0.4895, 0.5516, 0.4507, 0.6029, 0.4273, 0.4802,\n",
            "          0.4650, 0.5546],\n",
            "         [0.5004, 0.6997, 0.4995, 0.6138, 0.5236, 0.5950, 0.4472, 0.4629,\n",
            "          0.4225, 0.5406],\n",
            "         [0.6198, 0.7389, 0.4132, 0.6583, 0.4657, 0.5908, 0.4158, 0.5672,\n",
            "          0.3734, 0.5212],\n",
            "         [0.4488, 0.6042, 0.5806, 0.5883, 0.5069, 0.6619, 0.5269, 0.5487,\n",
            "          0.3520, 0.5573],\n",
            "         [0.6827, 0.7859, 0.4021, 0.4802, 0.3777, 0.6694, 0.3325, 0.3451,\n",
            "          0.6023, 0.6394],\n",
            "         [0.5550, 0.7657, 0.5798, 0.5720, 0.5934, 0.7111, 0.4215, 0.4267,\n",
            "          0.4529, 0.5851]],\n",
            "\n",
            "        [[0.4933, 0.5877, 0.6921, 0.6467, 0.5980, 0.5399, 0.5343, 0.5620,\n",
            "          0.3927, 0.7436],\n",
            "         [0.4387, 0.6129, 0.7246, 0.6349, 0.5442, 0.6610, 0.4733, 0.5918,\n",
            "          0.4789, 0.7721],\n",
            "         [0.4361, 0.6007, 0.7355, 0.6138, 0.5950, 0.6111, 0.4749, 0.5678,\n",
            "          0.4596, 0.7362],\n",
            "         [0.5030, 0.6403, 0.7180, 0.6198, 0.5695, 0.5733, 0.4734, 0.5310,\n",
            "          0.4046, 0.6877],\n",
            "         [0.4318, 0.6577, 0.7258, 0.6266, 0.4816, 0.6677, 0.5007, 0.5845,\n",
            "          0.5124, 0.7560],\n",
            "         [0.4964, 0.5561, 0.6864, 0.6564, 0.6336, 0.6013, 0.4616, 0.6702,\n",
            "          0.3878, 0.7765],\n",
            "         [0.5316, 0.6718, 0.7236, 0.5747, 0.5123, 0.5400, 0.4339, 0.4968,\n",
            "          0.4522, 0.7140],\n",
            "         [0.5118, 0.5911, 0.6731, 0.6011, 0.5629, 0.6041, 0.4584, 0.6380,\n",
            "          0.4557, 0.7758],\n",
            "         [0.4379, 0.5557, 0.6391, 0.6811, 0.5428, 0.6331, 0.5571, 0.6832,\n",
            "          0.4676, 0.7937],\n",
            "         [0.4664, 0.6107, 0.6176, 0.6032, 0.4969, 0.6409, 0.5024, 0.6852,\n",
            "          0.5490, 0.8088]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **6.3 BERT (Bidirectional Encoder Representations from Transformers):**\n",
        "#### Theory:\n",
        "- BERT is a pre-trained transformer model designed for bidirectional representation learning.\n",
        "- It utilizes masked language modeling and next sentence prediction during pre-training.\n",
        "- State-of-the-art performance in various NLP tasks.\n",
        "\n",
        "#### Code Example:"
      ],
      "metadata": {
        "id": "l2fr73zsBU7f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, BertForMaskedLM\n",
        "\n",
        "# Tokenize and mask input text\n",
        "# Instantiate a BERT tokenizer for the 'bert-base-uncased' model\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Input text to be tokenized and masked\n",
        "text = \"The cat is [MASK] on the mat.\"\n",
        "\n",
        "# Encode the input text using the BERT tokenizer\n",
        "# 'return_tensors' parameter specifies that PyTorch tensors should be returned\n",
        "encoded_tokens = tokenizer.encode(text, return_tensors='pt')  # Use encode directly\n",
        "\n",
        "# Load pre-trained BERT model for masked language modeling\n",
        "# Instantiate a BERT model for masked language modeling (MLM)\n",
        "model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Forward pass through the BERT model for masked language modeling\n",
        "# 'return_dict=True' returns a dictionary containing various model outputs\n",
        "output = model(encoded_tokens, return_dict=True)\n",
        "\n",
        "# Retrieve the predicted logits (scores) for masked tokens\n",
        "predictions = output.logits\n",
        "\n",
        "# Print the predicted logits\n",
        "print(predictions)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vJLq6sVaBZm3",
        "outputId": "2b3a417d-781c-424a-8a12-e410cff4e0a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[ -6.6352,  -6.5930,  -6.5885,  ...,  -5.9774,  -5.6839,  -4.0493],\n",
            "         [-14.5612, -14.3862, -14.4238,  ..., -12.1998, -11.9709, -11.0100],\n",
            "         [ -8.0677,  -8.1387,  -8.1146,  ...,  -7.2061,  -6.4629,  -6.5417],\n",
            "         ...,\n",
            "         [ -7.3555,  -7.8470,  -8.0578,  ...,  -7.2294,  -6.7105,  -5.0182],\n",
            "         [-10.6764, -10.2837, -10.5259,  ...,  -8.5979,  -9.6679,  -6.8525],\n",
            "         [-10.5542, -10.5132, -10.5281,  ...,  -9.9582,  -9.2576,  -5.6333]]],\n",
            "       grad_fn=<ViewBackward0>)\n"
          ]
        }
      ]
    }
  ]
}